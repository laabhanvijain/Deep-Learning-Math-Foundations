{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### **Q. 1 Bayesian Interpretation & Probability**\n",
        "\n",
        "**Scenario:** Your friend, Alex, claims he’s an expert dart thrower. He says he can hit the bull’s-eye on a dartboard with a probability of 0.7 on any given throw. You’re skeptical because you think an average person (including Alex, for all you know) typically has only about a 0.1 chance to hit the bull’s-eye. Initially, you believe there’s only a 1% chance Alex is actually as good as he claims.\n",
        "\n",
        "Now, Alex throws 5 darts in a row:\n",
        "\n",
        "Suppose he hits the bull’s-eye 3 times out of these 5 throws.\n",
        "\n",
        "**Prior Belief:** Write down your prior probability that Alex is truly an expert, i.e., $P(\\text{Expert})$.\n",
        "\n",
        "**Likelihoods:** Explain how you would compute the likelihood of observing 3 bull’s-eyes if Alex is an expert vs. if he’s not an expert.\n",
        "\n",
        "**Bayesian Update:** Use Bayes’ Theorem to find $P(\\text{Expert} \\mid 3 \\text{ bull’s-eyes in 5 throws})$.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- **( a )** Give a numerical value for the posterior (approximate is fine).\n",
        "- **( b )** Discuss how your belief changes from the prior to the posterior, and why.\n",
        "- **( c )** Explain what would happen if you changed your prior belief that Alex is an expert (e.g., from 1% to 20%). How sensitive is the final posterior to this change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyZMSYbx4Gl9"
      },
      "source": [
        "### **Answer 1**\n",
        "<br/>\n",
        "Given,\n",
        "<br/>\n",
        "Prior Probability of being an expert:$P(Expert)=0.01$\n",
        "<br/>\n",
        "Probability of hitting bull's eye given Alex is an expert: $P(H|E)=0.7$\n",
        "<br/>\n",
        "Probability of hitting bull's eye given Alex is not an expert: $P(H|E')=0.1$\n",
        "<br/>\n",
        "No: of dart throws=5\n",
        "<br/>\n",
        "No: of bull's eye-hit observed=3\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "1) $P(Expert)=0.01$ This is the prior beleif that Alex is truly an expert.\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "2) Using Binomial Distribution (because its either success or failure in hitting a bull-s eye:\n",
        "<br/>\n",
        "<br/>\n",
        "a)Alex is an expert:\n",
        "<br/> Likelihood of observing 3 bull's-eye given that Alex is an expert: $P(3\\hspace{0.2cm}bull's\\hspace{0.2cm}eyes\\hspace{0.2cm}in\\hspace{0.2cm} 5 \\hspace{0.2cm}throws|Expert)=\\binom{5}{3}(0.7)^3(0.3)^2$\n",
        "<br/>\n",
        "$=0.3087$\n",
        "<br/>\n",
        "<br/>\n",
        "b) a)Alex is not an expert:\n",
        "<br/> Likelihood of observing 3 bull's-eye given that Alex is not an expert:\n",
        "$P(3\\hspace{0.2cm} bull's\\hspace{0.2cm} eyes\\hspace{0.2cm} in\\hspace{0.2cm} 5 \\hspace{0.2cm}throws|Not\\hspace{0.2cm}an \\hspace{0.2cm} Expert )=\\binom{5}{3}(0.1)^3(0.9)^2$\n",
        "<br/>\n",
        "$=0.0081$\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "3) $P(Expert|3\\hspace{0.2cm} bull's\\hspace{0.2cm} eyes\\hspace{0.2cm} in\\hspace{0.2cm} 5 \\hspace{0.2cm}throws)=\\frac{P(3\\hspace{0.2cm}bull's\\hspace{0.2cm}eyes\\hspace{0.2cm}in\\hspace{0.2cm} 5 \\hspace{0.2cm}throws|Expert)P(Expert)}{P(3\\hspace{0.2cm} bull's\\hspace{0.2cm} eyes\\hspace{0.2cm} in\\hspace{0.2cm} 5 \\hspace{0.2cm}throws)}$\n",
        "<br/>\n",
        "$P(Expert | 3\\text{ bull's eyes in } 5 \\text{ throws}) =\n",
        "\\frac{P(3\\text{ bull's eyes in } 5 \\text{ throws} | Expert) P(Expert)}\n",
        "{P(3\\text{ bull's eyes in } 5 \\text{ throws} | Expert) P(Expert) + P(3\\text{ bull's eyes in } 5 \\text{ throws} | \\text{Not an Expert}) P(\\text{Not an Expert})}$\n",
        "<br/>\n",
        "$=\\frac{(0.3087)(0.01)}{(0.3087)(0.01)+(0.0081)(0.99)}$\n",
        "<br/>\n",
        "$=0.278$\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "4) **Aprroximate Posterior Probability**, as calculated above is:\n",
        "<br/>\n",
        "**$0.278$**\n",
        "<br/>\n",
        "<br/>\n",
        "**Change in Belief:**\n",
        "<br/>\n",
        "At the start, we had a prior belief of only 0.01, but an event in between made us shift our belief(0.01 to 0.278) that Alice might actually be an expert.This happened because initially we were very skeptical. But when he hit 3 bull's eyes out of 5 throws, we had to reconsider, the observed evidence thus led to change in our belief.\n",
        "<br/>\n",
        "<br/>\n",
        "**Sensitivity to Prior Belief:**\n",
        "<br/>\n",
        "If prior belief is changed significantly, i.e, from 1% to 20%, there will be a huge effect on posterior. This is because initially, we started very skeptically(very low prior), where the evidence shifted our belief moderately but when we already have a decent prior belief of Alice being an expert, this evidence will increase our belief with a greater amount.\n",
        "<br/>\n",
        "Mathematically:\n",
        "<br/>\n",
        "$=\\frac{(0.3087)(0.2)}{(0.3087)(0.2)+(0.0081)(0.8)}$\n",
        "<br/>\n",
        "$=0.905$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Q. 2 Estimating an Exponential Rate from Truncated Data**\n",
        "\n",
        "**Scenario** You run a bus service and record waiting times, $T_1, T_2, \\dots, T_n$, for buses at a particular stop. You assume each waiting time $T_i$ is exponentially distributed with some unknown rate $\\lambda$ (i.e., $T \\sim \\text{Exp}(\\lambda)$).\n",
        "\n",
        "However, your data-collecting device only starts recording when the waiting time is at least 10 minutes, so all observed $T_i$ are truncated from below at 10. In other words, you see only $T_i \\geq 10$.\n",
        "\n",
        "### Part A: MLE with Truncation\n",
        "\n",
        "- Write down the truncated likelihood:\n",
        "\n",
        "- Derive the probability density function for $T$ given that $T \\geq 10$.\n",
        "\n",
        "- Then write the likelihood function $L(\\lambda)$ for your observed data $\\{t_1, \\dots, t_n\\}$, each $\\geq 10$.\n",
        "\n",
        "- Find the MLE:\n",
        "\n",
        " - Take the log of the truncated likelihood (the log-likelihood).\n",
        " \n",
        " - Differentiate with respect to $\\lambda$, set it to zero, and solve for the MLE $\\hat{\\lambda}$.\n",
        "\n",
        "- Discuss briefly why ignoring truncation would give an incorrect estimate.\n",
        "\n",
        "### (Out-of-the-box reflection):\n",
        "\n",
        "Suppose, in practice, a few data points might be missing because the device occasionally fails to start at exactly 10 minutes. How might this complicate your MLE procedure? (No need for a full derivation, just give a conceptual discussion.)\n",
        "\n",
        "### Part B: MAP Estimation with a Gamma Prior\n",
        "\n",
        "**Prior Assumption:** Suppose you place a Gamma($\\alpha$, $\\beta$) prior on $\\lambda$.\n",
        "\n",
        "- Write the (unnormalized) posterior $p(\\lambda \\mid \\text{data})$ that incorporates this truncated-likelihood model.\n",
        "\n",
        "- Find the MAP Estimate:\n",
        "\n",
        " - Derive the expression for $\\hat{\\lambda}_{\\text{MAP}}$ by maximizing the log-posterior.\n",
        "\n",
        "- Compare this estimate to your MLE in Part A. Under what conditions do they differ significantly?\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "- Explain, in practical terms, how adding this prior (e.g., based on historical data about bus arrivals) can pull your estimate of $\\lambda$ away from the pure MLE.\n",
        "\n",
        "- In what scenario might you prefer the MAP estimate over the MLE for your bus scheduling decisions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGFjL4HWFiue"
      },
      "source": [
        "### **Answer 2**\n",
        "<br/>\n",
        "PART A:\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "1. Truncated PDF:\n",
        "<br/>\n",
        "$P(T≥10)=\\int_{10}^{∞}λe^{-λt}dt$\n",
        "<br/>\n",
        "$=e^{-10λ}$\n",
        "<br/>\n",
        "$P(T|T≥10)=\\frac{λe^{-λt}}{e^{-10λ}}$\n",
        "<br/>\n",
        "$=λe^{-λ(t-10)}$\n",
        "<br/>\n",
        "Likelihood function $L(λ)=Π_{i=1}^{n}λe^{-λ(t_i-10)}$\n",
        "\n",
        "\n",
        "2.  Finding the MLE:\n",
        "<br/>\n",
        "log-likelihood:\n",
        "<br/>\n",
        "$log(L(λ))=∑_{i=1}^{n}logλ-λ∑_{i=1}^{n}(t_i-10)$\n",
        "<br/>\n",
        "$=nlogλ-λ∑_{i=1}^{n}(t_i-10)$\n",
        "<br/>\n",
        "Differentiating both sides wrt $λ$ and equating to 0.\n",
        "<br/>\n",
        "$=\\frac{n}{λ}-∑_{i=1}^{n}(t_i-10)=0$\n",
        "<br/>\n",
        "$λ_{MLE}^{'}=\\frac{n}{∑_{i=1}^{n}(t_i-10)}$\n",
        "\n",
        "\n",
        "3. Ignoring truncation would lead to an incorrect estimate of $λ$ because the observed data does not include values below 10. If we do not consider the fact, that there are no values less than 10, this would understimate $λ$ as we are missing shorter waiting times.\n",
        "The $λ$ will then be equal to $\\frac{1}{T}$\n",
        "\n",
        "\n",
        "\n",
        "4. If some data points are missing beacause of failure of device, if this occurs randomly, MLE won't get affected, but let's say there is some pattern in the failure, then MLE will led to incoorect prediction of $λ$\n",
        "\n",
        "**PART B:**\n",
        "\n",
        "1. Gamma distribution:\n",
        "<br/>\n",
        "$f(λ)=\\frac{1}{β^αΓ{α}}λ^{α-1}e^{-\\frac{λ}{β}}$\n",
        "<br/>\n",
        "$p(λ|data)∝p(λ)L(λ)$\n",
        "<br/>\n",
        "$p(λ)L(λ)=Π_{i=1}^{n}λe^{-λ(t_i-10)}\\frac{1}{β^αΓ{α}}λ^{α-1}e^{-\\frac{λ}{β}}$\n",
        "<br/>\n",
        "$=λ^{n+α-1}e^{-λ(∑_{i=1}^{n}(t_i-10)+ \\frac{1}{β}) }$\n",
        "\n",
        "<br/>\n",
        "\n",
        "2. Taking log :\n",
        "<br/>\n",
        "$=(n+α-1)logλ{-λ(∑_{i=1}^{n}(t_i-10)+ \\frac{1}{β}) }$\n",
        "<br/>\n",
        "Differentiating both sides wrt $λ$ and equating to 0.\n",
        "<br/>\n",
        "$\\frac{n+α-1}{λ}-(∑_{i=1}^{n}(t_i-10)+ \\frac{1}{β})=0$\n",
        "<br/>\n",
        "$λ_{MAP}^{'}=\\frac{n+α-1}{∑_{i=1}^{n}(t_i-10)+ \\frac{1}{β}}$\n",
        "<br/>\n",
        "<br/>\n",
        "Now, we have the two expressions:\n",
        "<br/>\n",
        "$λ_{MLE}^{'}=\\frac{n}{∑_{i=1}^{n}(t_i-10)}$\n",
        "<br/>\n",
        "$λ_{MAP}^{'}=\\frac{n+α-1}{∑_{i=1}^{n}(t_i-10)+ \\frac{1}{β}}$\n",
        "<br/>\n",
        "If $α→1, β→∞, λ_{MLE}∼λ_{MAP}$\n",
        "<br/>\n",
        "If, $α$ is large, prior will dominate, hence the estimate of MAP will differ significantly from MLE.\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "3. Practical Impact:\n",
        "<br/>\n",
        "The prior can shift the estimate of $λ$ Let's say historical data suggests λ is around 0.8 but new data is different it varies a lot , involving prior is a better choice, and so at that time the MAP estimate won’t deviate too much from 0.8, but in case of MLE, there is a possibility that $λ$ if very far from 0.8, which should not have been the case since historical data matters.\n",
        "<br/>\n",
        "\n",
        "When historical trends matter like in bus scheduling, past patterns are useful for decision-making and so prior should be involved, hence we should prefer MAP estimate over MLE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Q. 3 KL Divergence & Basic Properties**\n",
        "\n",
        "**Scenario:** In many ML and DL setups (e.g., classification, variational inference), we compare two probability distributions $P$ and $Q$. For a discrete space $\\{1, 2, \\dots, k\\}$, the KL divergence is defined as\n",
        "\n",
        "$$\n",
        "D_{\\text{KL}}(P \\parallel Q) = \\sum_{i=1}^{k} p_i \\log\\left(\\frac{p_i}{q_i}\\right).\n",
        "$$\n",
        "\n",
        "### Non-Negativity\n",
        "\n",
        "- Show (or convincingly outline why) $D_{\\text{KL}}(P \\parallel Q) \\geq 0$.\n",
        "\n",
        "- Under what condition is $D_{\\text{KL}}(P \\parallel Q) = 0$? \n",
        " (Hint: You can use the concept that $\\log$ is a concave function or refer to known inequalities.)\n",
        "\n",
        "### Connection to Cross-Entropy\n",
        "\n",
        "Recall the definition of cross-entropy between two distributions $P$ and $Q$:\n",
        "\n",
        "$$\n",
        "H(P, Q) = - \\sum_{i=1}^{k} p_i \\log(q_i).\n",
        "$$\n",
        "\n",
        "- Show that minimizing $D_{\\text{KL}}(P \\parallel Q)$ with respect to $Q$ is essentially the same as minimizing the cross-entropy $H(P, Q)$, up to an additive constant that depends only on $P$.\n",
        "\n",
        "### (Optional) Coding Exploration\n",
        "\n",
        "Write a short Python script to:\n",
        "\n",
        "1. Define a “true” distribution $P$ on $\\{1, \\dots, k\\}$.\n",
        "2. Define several candidate distributions $Q_1, Q_2, \\dots$ (e.g., random guesses).\n",
        "3. Compute both $D_{\\text{KL}}(P \\parallel Q_i)$ and the cross-entropy $H(P, Q_i)$.\n",
        "4. Illustrate numerically that the candidate $Q$ that gives the smallest cross-entropy also gives the smallest KL divergence.\n",
        "\n",
        "### Reflection\n",
        "\n",
        "Explain why in machine learning classification problems, minimizing the average cross-entropy (or equivalently the KL divergence from the ground truth) is preferred over simply trying to maximize accuracy directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iek8KmLv4SIY"
      },
      "source": [
        "### **Answer 3**\n",
        "\n",
        "<br/>\n",
        "1. $D_{KL}(P||Q)=∑_{i=1}^kp_ilog(\\frac{p_i}{q_i})$\n",
        "<br/>\n",
        "By Jennsen's inequality, we know that for a convex function:\n",
        "<br/>\n",
        "$f(E(X))≤E(f(X))$\n",
        "<br/>\n",
        "We know that $-log(x)$ is a convex fnction, so:\n",
        "<br/>\n",
        "$-E_p(log(\\frac{q}{p}))\\geq -log(E_p[log\\frac{q}{p}])$\n",
        "<br/>\n",
        "$-E_p(log(\\frac{q}{p}))\\geq -log(\\int p(x).\\frac{q(x)}{p(x)}dx)$\n",
        "<br/>\n",
        "$-E_p(log(\\frac{q}{p}))\\geq -log(\\int q(x)dx)$\n",
        "<br/>\n",
        "$-E_p(log(\\frac{q}{p}))\\geq -log(1)$\n",
        "<br/>\n",
        "$-E_p(log(\\frac{q}{p}))\\geq 0$\n",
        "<br/>\n",
        "$∑_{i=1}^kp_ilog(\\frac{p_i}{q_i})\\geq 0$\n",
        "<br/>\n",
        "<br/>\n",
        "KL Divergence is zero, when $q_i=p_i$, This makes sense because when predicted and actual distribution is same, there is no error, hence it must be zero.\n",
        "\n",
        "\n",
        "2. Cross Entropy:\n",
        "<br/>\n",
        "$H(P,Q)=-\\sum_{i=1}^{k}p_ilog(q_i)$\n",
        "<br/>\n",
        "$D_{KL}(P||Q)=∑_{i=1}^kp_ilog(\\frac{p_i}{q_i})$\n",
        "<br/>\n",
        "$D_{KL}(P||Q)=∑_{i=1}^kp_ilog(p_i)-∑_{i=1}^kp_ilog(q_i)$\n",
        "<br/>\n",
        "$D_{KL}(P||Q)=-H(P)+H(P,Q)$\n",
        "<br/>\n",
        "$H(P)$ only depends on $P$, and so is a constant with respect to $Q$\n",
        "<br/>\n",
        "$D_{KL}(P||Q)=H(P,Q)+c$\n",
        "<br/>\n",
        "Hence, minimising KL Divergence with respect to $Q$ is same as minimising cross-entropy.\n",
        "\n",
        "\n",
        "3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SL0VU-qt6nYQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "etMbBToK7I5t"
      },
      "outputs": [],
      "source": [
        "def prob_dist(k):\n",
        "    probs = np.random.rand(k) #won't be a probab distribution\n",
        "    return probs / probs.sum() #normalising it\n",
        "\n",
        "def kl_divergence(P, Q):\n",
        "    return np.sum(P * np.log(P / Q))\n",
        "\n",
        "def cross_entropy(P, Q):\n",
        "    return -np.sum(P * np.log(Q))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pZBk5MZV7nhv"
      },
      "outputs": [],
      "source": [
        "k=20\n",
        "P = prob_dist(k)\n",
        "candidate_distributions = 20\n",
        "candidates =[]\n",
        "for _ in range(candidate_distributions):\n",
        "    candidates.append(prob_dist(k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4kgF7pfX8zFb"
      },
      "outputs": [],
      "source": [
        "kl_values = []\n",
        "cross_entropy_values = []\n",
        "for Q in candidates:\n",
        "    kl_values.append(kl_divergence(P, Q))\n",
        "    cross_entropy_values.append(cross_entropy(P, Q))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDlyc0hO9FTh",
        "outputId": "d2fb2ce8-a6e6-4692-848a-e288fee34a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KL Divergence values: [np.float64(0.4842338918081053), np.float64(0.4017598403374807), np.float64(0.41599220935805153), np.float64(0.5796575985398278), np.float64(0.21444723502690194), np.float64(0.40150494965147604), np.float64(0.4053703625693927), np.float64(0.43580999698637585), np.float64(0.6248315026386552), np.float64(0.5415134815541216), np.float64(0.682275311836277), np.float64(0.37220390716704377), np.float64(0.8122787036146685), np.float64(0.21666049113224237), np.float64(0.773095089394467), np.float64(0.4777807542841701), np.float64(0.38958562571331046), np.float64(0.6160300806791499), np.float64(0.3890747797630317), np.float64(0.3528242408951837)]\n",
            "Cross-Entropy values: [np.float64(3.351429316864193), np.float64(3.268955265393568), np.float64(3.2831876344141397), np.float64(3.4468530235959154), np.float64(3.0816426600829896), np.float64(3.268700374707564), np.float64(3.2725657876254797), np.float64(3.3030054220424634), np.float64(3.4920269276947424), np.float64(3.408708906610209), np.float64(3.5494707368923644), np.float64(3.239399332223132), np.float64(3.6794741286707566), np.float64(3.08385591618833), np.float64(3.640290514450555), np.float64(3.3449761793402573), np.float64(3.2567810507693977), np.float64(3.4832255057352377), np.float64(3.2562702048191197), np.float64(3.220019665951271)]\n",
            "Minimum KL Divergence: {min_kl_value} at index {min_kl_index}\n",
            "Minimum Cross-Entropy: {min_ce_value} at index {min_ce_index}\n",
            "The candidate that minimizes Cross-Entropy also minimizes KL Divergence!\n"
          ]
        }
      ],
      "source": [
        "# Printing arrays\n",
        "print(\"KL Divergence values:\", kl_values)\n",
        "print(\"Cross-Entropy values:\", cross_entropy_values)\n",
        "\n",
        "min_kl_index = np.argmin(kl_values)\n",
        "min_kl_value = kl_values[min_kl_index]\n",
        "\n",
        "min_ce_index = np.argmin(cross_entropy_values)\n",
        "min_ce_value = cross_entropy_values[min_ce_index]\n",
        "\n",
        "# printing minimum of both\n",
        "print(\"Minimum KL Divergence: {min_kl_value} at index {min_kl_index}\")\n",
        "print(\"Minimum Cross-Entropy: {min_ce_value} at index {min_ce_index}\")\n",
        "\n",
        "# checking index\n",
        "if min_kl_index == min_ce_index:\n",
        "    print(\"The candidate that minimizes Cross-Entropy also minimizes KL Divergence!\")\n",
        "else:\n",
        "    print(\"The candidates are different.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNnFNr2Ub8S6Tljp/AnuQ8i",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
