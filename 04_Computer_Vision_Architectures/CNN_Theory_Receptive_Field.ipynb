{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Q7** \n",
        "Consider a single 2D convolutional layer (without activation) that takes an input tensor\n",
        "\n",
        "$$\n",
        "X \\in \\mathbb{R}^{C_{\\text{in}} \\times H_{\\text{in}} \\times W_{\\text{in}}}\n",
        "$$\n",
        "\n",
        "(with $C_{\\text{in}}$ channels, height $H_{\\text{in}}$, width $W_{\\text{in}}$) and produces an output tensor\n",
        "\n",
        "$$\n",
        "Y \\in \\mathbb{R}^{C_{\\text{out}} \\times H_{\\text{out}} \\times W_{\\text{out}}}\n",
        "$$\n",
        "\n",
        "by convolving $X$ with a set of learnable kernels\n",
        "\n",
        "$$\n",
        "W \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times K_h \\times K_w}\n",
        "$$\n",
        "\n",
        "and adding biases $b \\in \\mathbb{R}^{C_{\\text{out}}}$. The convolution is performed with integer stride $(s_h,\\,s_w)$ and zero-padding $(p_h,\\,p_w)$ on the height and width dimensions, respectively.\n",
        "\n",
        "---\n",
        "\n",
        "### (a) Convolution as Matrix Multiplication (im2col)\n",
        "\n",
        "1. Let us “unroll” each local $K_h \\times K_w$ patch of $X$ into a column vector of size $(C_{\\text{in}}\\;K_h\\;K_w)$.\n",
        "\n",
        "    **(i)** Describe precisely how many such columns there are (i.e., the number of receptive-field locations), and hence specify the dimensions of the resulting “patch matrix” $X_{\\text{im2col}}$.\n",
        "\n",
        "    **(ii)** Show that you can reshape $W$ into a 2D matrix\n",
        "\n",
        "    $$\n",
        "    W_{\\text{mat}} \\;\\in\\; \\mathbb{R}^{C_{\\text{out}} \\,\\times\\, \\bigl(C_{\\text{in}}\\,K_h\\,K_w\\bigr)}\n",
        "    $$\n",
        "\n",
        "    so that the convolution $Y$ (before bias addition) is exactly\n",
        "\n",
        "    $$\n",
        "    Y_{\\text{flat}} \\;=\\; W_{\\text{mat}}\\;X_{\\text{im2col}},\n",
        "    $$\n",
        "\n",
        "    where $Y_{\\text{flat}}$ is a $C_{\\text{out}} \\times (H_{\\text{out}}\\,W_{\\text{out}})$ matrix whose columns correspond to each spatial output location.\n",
        "\n",
        "    **(iii)** Write down the explicit shapes of $X_{\\text{im2col}}$, $W_{\\text{mat}}$, and $Y_{\\text{flat}}$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### (b) Receptive Field Across Multiple Layers\n",
        "\n",
        "Suppose you stack $L$ convolutional layers (without any intervening pooling or striding modifications aside from what’s specified) where layer $i$ has:\n",
        "- kernel size $\\bigl(K^{(i)},\\,K^{(i)}\\bigr)$,\n",
        "- stride $\\bigl(s^{(i)},\\,s^{(i)}\\bigr)$,\n",
        "- padding $\\bigl(p^{(i)},\\,p^{(i)}\\bigr)$.\n",
        "\n",
        "For simplicity, assume all convolutions are square and have the same hyperparameters in height and width (i.e., $K^{(i)} = K^{(i)} = K^{(i)}$, and similarly $s^{(i)} = s^{(i)} = s^{(i)}$, $p^{(i)} = p^{(i)} = p^{(i)}$), and let’s focus on the vertical dimension (height) alone (the horizontal dimension is identical by symmetry).\n",
        "\n",
        "\n",
        "1. **Define** the receptive-field size $R^{(1)}$ of a single neuron in the first convolutional layer with respect to the input height.\n",
        "\n",
        "2. **Provide** a recursive formula for the effective receptive-field $R^{(i)}$ of a neuron in layer $i$ (for $i = 2,\\,3,\\,\\dots,\\,L$) in terms of $R^{(i-1)}$, $K^{(i)}$, $s^{(i)}$, and $p^{(i)}$. \n",
        "    *(Padding does not affect the size of the receptive field, only the relative position.)*\n",
        "\n",
        "3. **Derive** a closed-form expression for $R^{(L)}$ (i.e., the number of contiguous input-pixels in $X$ that influence a single output activation in layer $L$) as a function of all $\\{\\,K^{(j)},\\,s^{(j)},\\,p^{(j)}\\}_{j=1}^L$.\n",
        "\n",
        "4. **Interpretation:** If all strides $s^{(j)} = 1$ and paddings satisfy $p^{(j)} = \\lfloor K^{(j)}/2 \\rfloor$ (“same” convolution), show that\n",
        "    $$\n",
        "    R^{(L)} \\;=\\; 1 \\;+\\; \\sum_{j=1}^{L} \\bigl(K^{(j)} - 1\\bigr).\n",
        "    $$\n",
        "    Explain why this matches the intuitive notion of “stacking” kernels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l38bY5BWG23T"
      },
      "source": [
        "# **7 a)**\n",
        "i) The number of receptive field locations which is basically the number of columns.\n",
        "<br/>\n",
        "$H_{out}=\\frac{H_{in}-K_{h}+2p_h}{s_h}+1$\n",
        "<br/>\n",
        "<br/>\n",
        "$W_{out}=\\frac{W_{in}-K_{w}+2p_w}{s_w}+1$\n",
        "<br/>\n",
        "<br/>\n",
        "When we place kernel on input each patch will give one value as an output which is one receptive field. The number of receptive fields is basically equal to the output we get i.e $H_{out} W_{out}$\n",
        "<br/>\n",
        "We unroll the patch into a column vector of size $C_{in}K_h K_w$ and no: of the colums is $H_{out} \\times W_{out}$.\n",
        "<br/>\n",
        "<br/>\n",
        "$X_{im2col} \\in \\mathbb{R}^{C_{in}K_h K_w \\times H_{out} W_{out}} $\n",
        "<br/>\n",
        "<br/>\n",
        "<br/>\n",
        "ii) $W \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times K_h \\times K_w}$ which basically means there $C_{out}$ number of filters of the size $C_{in}\\times K_h \\times K_w$ After unrolling the filters into $C_{in}K_h K_w$, we take each kernel and stack this row vector to form a final weight matrix, the shape changes to $W_{\\text{mat}} \\;\\in\\; \\mathbb{R}^{C_{\\text{out}} \\,\\times\\, \\bigl(C_{\\text{in}}\\,K_h\\,K_w\\bigr)}$\n",
        "<br/>\n",
        "<br/>\n",
        "<br/>\n",
        "iii) The shape of all these is already given in the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn09kzpcSbDN"
      },
      "source": [
        "# **7 b)**\n",
        "1) Receptive field of neuron in first conv layer is the Kernel's height.\n",
        "<br/>\n",
        "<br/>\n",
        "Referred blog\n",
        "<br/>\n",
        "2) $R_i= R_{i-1} + (k_i-1) ∏_{j=1}^{i-1}s_j$\n",
        "<br/>\n",
        "<br/>\n",
        "3) $R_i= R_{i-1} + (k-1) ∏_{j=1}^{i-1}s_j$\n",
        "<br/>\n",
        "$R_{i+1}= R_{i} + (k-1) ∏_{j=1}^{i}s_j$\n",
        "<br/>\n",
        "$R_{i+2}= R_{i+1} + (k-1) ∏_{j=1}^{i+1}s_j$\n",
        "<br/>\n",
        "Putting the values:\n",
        "<br/>\n",
        "$R_{L}= R_{0} + ∑_1^L(k_i-1) ∏_{j=1}^{i}s_j$\n",
        "<br/>\n",
        "In the start without applying any layer receptive field is 1, so:\n",
        "<br/>\n",
        "$R_{L}= 1 + ∑_1^L(k_i-1) ∏_{j=1}^{i}s_j$\n",
        "<br/>\n",
        "<br/>\n",
        "4) When stride=1\n",
        "<br/>\n",
        "$R_{L} = 1 + ∑_1^L(k_i-1) $\n",
        "<br/>\n",
        "Couldn't get a proper intution of this.\n",
        "I understood it something like initially without any conv layer the receptive field was 1, Then it became $k$ after 1 conv layer, which can be written as $1+(k-1)$. Adding more convolutional layers will add $k$ into receptive field, but why subtract 1, didn't get that.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
