{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q.5**\n",
        "A string of parentheses—consisting only of “(” and “)” characters—is called well-formed (or balanced) if every opening parenthesis “(” is eventually closed by a matching “)” in the correct order. For e.g.:\n",
        "\n",
        "*Well-formed:*\n",
        "\n",
        "> **(), (()), ()(), (()(()))**\n",
        "\n",
        "*Not well-formed:*\n",
        "\n",
        "> **)(, ((), ())(, (()))(**\n",
        "\n",
        "Your task is to build a model that, given a fixed-length input of parentheses (e.g., length = 10), outputs a binary label (0 or 1):\n",
        "\n",
        "**1** if the input string is well-formed, **0** if the input string is not well-formed.\n",
        "\n",
        "Fix a sequence length L = 10 (or any even length you choose, e.g., 12 or 14). Also, generate a large set of random parentheses strings of length L (each character is randomly chosen to be “**(**” or “**)**”).For each generated string, write a small function to check if it’s balanced (I'll give it below):\n",
        "\n",
        "```python\n",
        "def is_balanced(s):\n",
        "    count = 0\n",
        "    for ch in s:\n",
        "        if ch == \"(\":\n",
        "            count += 1\n",
        "        else: # ch == \")\"\n",
        "            count -= 1\n",
        "        if count < 0:\n",
        "            return False\n",
        "    return (count == 0)\n",
        "```\n",
        "Encode “(”→ 0 & “)”→ 1 (optionally with a small embedding) and build a model with one LSTM layer (e.g., 16–32 hidden units) followed by a dense layer with sigmoid activation to output the probability of balancedness. Train with binary cross‐entropy and an optimizer like Adam (you can test on others as well) until both training and validation accuracies are good enough. Can refer RNN-LSTM ke yt videos if code krne me problems ho, i would highly recommend, self code the problem !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2wzggVj83p3i"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rw4wbmCfgxn8"
      },
      "outputs": [],
      "source": [
        "#Hyper parameters\n",
        "num_epochs=4\n",
        "seq_length=10\n",
        "learning_rate=0.001\n",
        "input_size=1\n",
        "hidden_size=32\n",
        "batch_size=100\n",
        "num_class=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gc32KhrTbHu2"
      },
      "outputs": [],
      "source": [
        "def is_balanced(s):\n",
        "    count = 0\n",
        "    for ch in s:\n",
        "        if ch == 0:\n",
        "            count += 1\n",
        "        else:  # ch == \")\"\n",
        "            count -= 1\n",
        "        if count < 0:\n",
        "            return False\n",
        "    return (count == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hiOH3lnkimaX"
      },
      "outputs": [],
      "source": [
        "def dataset(num_sampl, seq_length=seq_length):\n",
        "  sequences=[]\n",
        "  labels=[]\n",
        "  #encoding 0 and 1 to (, )\n",
        "  for i in range(num_sampl):\n",
        "    sequence = [random.randint(0, 1) for num in range(seq_length)]\n",
        "    label= is_balanced(sequence)\n",
        "\n",
        "    sequences.append(sequence)\n",
        "    labels.append(label)\n",
        "  return torch.tensor(sequences, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i-E79CmIcR93"
      },
      "outputs": [],
      "source": [
        "train_sequence, train_label= dataset(num_sampl=8000 )\n",
        "test_sequence, test_label=dataset(num_sampl=2000 )\n",
        "\n",
        "train_dataset = TensorDataset(train_sequence, train_label)\n",
        "test_dataset = TensorDataset(test_sequence, test_label)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6AsOk0gZlBNv"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_class,num_layers=1 ):\n",
        "     super(LSTM, self).__init__()\n",
        "     self.hidden_size=hidden_size\n",
        "     self.num_layers=num_layers\n",
        "\n",
        "     self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "     self.fc = nn.Linear(hidden_size, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0,c0))\n",
        "        out = out[:, -1, :] #last hidden state\n",
        "        out = torch.sigmoid(self.fc(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hG7g3pLImRuf"
      },
      "outputs": [],
      "source": [
        "model = LSTM(input_size, hidden_size, num_class)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPNhjbDgmeFF",
        "outputId": "61bcce79-cda2-433e-bcfd-19cb4cf94765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/4], Step [10/80], Loss: 0.6607\n",
            "Epoch [1/4], Step [20/80], Loss: 0.6028\n",
            "Epoch [1/4], Step [30/80], Loss: 0.5336\n",
            "Epoch [1/4], Step [40/80], Loss: 0.3989\n",
            "Epoch [1/4], Step [50/80], Loss: 0.2362\n",
            "Epoch [1/4], Step [60/80], Loss: 0.1547\n",
            "Epoch [1/4], Step [70/80], Loss: 0.0799\n",
            "Epoch [1/4], Step [80/80], Loss: 0.2327\n",
            "Epoch [2/4], Step [10/80], Loss: 0.2666\n",
            "Epoch [2/4], Step [20/80], Loss: 0.1369\n",
            "Epoch [2/4], Step [30/80], Loss: 0.1063\n",
            "Epoch [2/4], Step [40/80], Loss: 0.1683\n",
            "Epoch [2/4], Step [50/80], Loss: 0.1365\n",
            "Epoch [2/4], Step [60/80], Loss: 0.1364\n",
            "Epoch [2/4], Step [70/80], Loss: 0.2629\n",
            "Epoch [2/4], Step [80/80], Loss: 0.1681\n",
            "Epoch [3/4], Step [10/80], Loss: 0.1993\n",
            "Epoch [3/4], Step [20/80], Loss: 0.0736\n",
            "Epoch [3/4], Step [30/80], Loss: 0.1677\n",
            "Epoch [3/4], Step [40/80], Loss: 0.3234\n",
            "Epoch [3/4], Step [50/80], Loss: 0.1982\n",
            "Epoch [3/4], Step [60/80], Loss: 0.1981\n",
            "Epoch [3/4], Step [70/80], Loss: 0.1370\n",
            "Epoch [3/4], Step [80/80], Loss: 0.2294\n",
            "Epoch [4/4], Step [10/80], Loss: 0.3205\n",
            "Epoch [4/4], Step [20/80], Loss: 0.0762\n",
            "Epoch [4/4], Step [30/80], Loss: 0.1664\n",
            "Epoch [4/4], Step [40/80], Loss: 0.1045\n",
            "Epoch [4/4], Step [50/80], Loss: 0.1357\n",
            "Epoch [4/4], Step [60/80], Loss: 0.1051\n",
            "Epoch [4/4], Step [70/80], Loss: 0.1352\n",
            "Epoch [4/4], Step [80/80], Loss: 0.1658\n"
          ]
        }
      ],
      "source": [
        "#Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (sequences, labels) in enumerate(train_loader):\n",
        "        sequences = sequences.reshape(-1, seq_length, input_size)\n",
        "        outputs = model(sequences)\n",
        "        labels=labels.view(-1,1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH_8Ki3hvIHS",
        "outputId": "f87adb48-43b3-48e8-b10f-ff9a20c8d1cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network: 96.1 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences = sequences.reshape(-1, seq_length, input_size)\n",
        "        labels = labels.view(-1, 1)\n",
        "        outputs = model(sequences)\n",
        "\n",
        "        predicted = torch.round(outputs.data)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
