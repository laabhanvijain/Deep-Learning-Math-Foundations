{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q.4** What Does Depth Buy Us?\n",
        "\n",
        "You will implement a mini Transformer encoder with a variable number of self-attention layers (e.g., 1, 2, or 4). Each layer consists of\n",
        "1. A single-head self attention mechanism\n",
        "2. A simple feedforward layer\n",
        "3. Residual Connections (optional but encouraged)\n",
        "\n",
        "**Task**:\n",
        "- Implement the encoder from scratch in PyTorch.\n",
        "- Create a toy input sequence of token embeddings (e.g., a fixed random tensor of shape [seq_len, d_model]).\n",
        "- Pass the same input sequence through 1-layer, 2-layer, and 4-layer self-attention stacks.\n",
        "- Visualize the attention weights for one token (e.g., the middle token) across all layers.\n",
        "- For one token (e.g. the middle token), plot how its attention distribution evolves with depth.\n",
        "\n",
        "**In your own words, explain**:\n",
        "- Does deeper attention \"sharpen\", \"blur\", or \"redirect\" focus?\n",
        "- How is meaning transformed across layers?\n",
        "\n",
        "> You are free to use a fixed positional encoding (e.g., sinusoidal) or none at all. The goal is to study qualitative changes in attention with increasing depth.\n",
        "\n",
        "```python\n",
        "# Suggested structure\n",
        "class SelfAttentionLayer(nn.Module):\n",
        " ...\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        " def __init__(self, num_layers):\n",
        " ...\n",
        "\n",
        "# Use random or fixed embeddings for a toy example input sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "elxOh-xYcgtt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFA4I27jGdc9",
        "outputId": "066b0470-b967-470c-ce55-48595d722f33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 64])\n",
            "tensor([[-1.6114e-01, -1.3445e+00, -1.4474e+00,  2.0271e+00,  6.6079e-01,\n",
            "          5.4417e-01, -1.6611e-01,  9.2867e-02, -1.4756e+00,  7.7923e-01,\n",
            "          9.2904e-01, -1.2588e-01,  1.5676e+00,  9.5272e-01,  5.4230e-01,\n",
            "         -2.4356e-01, -1.2689e+00,  1.7080e-01,  1.3478e+00,  6.2493e-01,\n",
            "         -1.1023e+00,  1.6912e-01, -7.8394e-01,  2.8224e+00,  2.2944e+00,\n",
            "          3.9569e-01,  2.3372e+00, -7.2517e-02,  5.3807e-01, -2.7931e-01,\n",
            "          1.2905e-01,  9.1313e-01, -5.1254e-01,  6.0260e-01, -1.1700e-01,\n",
            "         -2.8920e-03,  1.0927e+00, -4.7146e-01,  2.9892e+00,  1.1947e-01,\n",
            "          5.4790e-01, -1.8880e+00, -1.5808e+00,  1.7201e+00, -8.7866e-01,\n",
            "         -1.1480e+00,  9.2404e-02, -1.4438e-01,  5.5407e-02,  6.5202e-02,\n",
            "          1.9071e+00, -4.5858e-01,  8.3533e-01,  2.7186e-01,  1.2193e+00,\n",
            "          2.2775e-01,  1.7512e-01, -1.3678e+00, -6.9906e-01,  3.8462e-03,\n",
            "         -6.2365e-02, -1.4455e-03,  1.7665e+00,  8.4656e-01],\n",
            "        [ 5.4657e-01, -2.0432e+00, -8.4425e-01,  9.0125e-01,  3.2302e-01,\n",
            "          8.0062e-01,  9.1112e-01,  1.3540e+00, -1.4635e+00,  3.5412e-01,\n",
            "         -2.1222e+00, -1.7059e-01,  1.4771e+00, -1.2887e+00, -1.3092e+00,\n",
            "         -1.8886e-01,  2.3093e-01,  1.3091e-01, -2.3742e-02,  6.9284e-01,\n",
            "         -3.5542e-01,  8.5079e-02,  4.8223e-03, -1.0415e-02, -7.6711e-01,\n",
            "          1.1708e+00, -7.9797e-01,  1.9431e-01, -2.3392e-01, -9.8492e-01,\n",
            "         -1.1273e+00,  1.6221e-01,  7.6635e-01,  1.1264e+00,  1.2469e+00,\n",
            "          3.4986e-01,  1.1259e+00, -2.7938e-01,  1.2105e-01,  5.4710e-01,\n",
            "          6.1665e-01,  4.7358e-01,  3.6122e-01,  9.8757e-01, -4.1667e-01,\n",
            "          2.2055e-02,  4.7867e-01,  2.2187e-01,  9.0165e-01,  9.4973e-01,\n",
            "          8.8326e-01,  7.6022e-01,  5.0745e-01, -1.0362e+00,  7.0685e-01,\n",
            "          1.2843e-01, -2.6136e+00,  5.5248e-01,  1.5027e-01,  8.8529e-01,\n",
            "         -2.2079e+00, -1.0543e+00, -8.7517e-01, -1.8576e-01],\n",
            "        [ 1.0079e+00,  8.0073e-01,  7.5937e-01,  1.1505e-01,  5.8716e-01,\n",
            "         -7.4244e-01,  6.9824e-02,  2.0544e-01, -8.8067e-01, -3.9795e-01,\n",
            "          3.6263e-01, -1.6505e+00, -5.0750e-01,  1.6321e+00,  1.0959e-01,\n",
            "          2.8020e-01, -1.0232e+00,  7.7068e-01, -1.0478e+00, -3.6218e-01,\n",
            "          7.5147e-01,  1.6500e+00,  4.3676e-01, -1.5269e-01,  1.5939e+00,\n",
            "         -1.6166e-01,  7.5493e-01, -7.7919e-01, -1.7449e-01,  1.6133e-01,\n",
            "         -5.3697e-01, -1.3725e+00,  1.0908e-01,  1.3983e+00, -1.9176e-01,\n",
            "         -1.2707e+00,  4.2518e-01, -1.4808e+00,  2.4083e+00, -2.9157e-01,\n",
            "          1.3389e+00, -9.8297e-01,  4.1049e-01, -1.2459e+00, -8.2998e-01,\n",
            "         -1.0562e-01, -1.1187e+00, -2.3624e+00, -9.5706e-01, -4.6661e-01,\n",
            "         -6.8628e-01, -1.1408e+00, -2.9843e-01, -6.3296e-01, -1.6326e+00,\n",
            "         -4.2282e-02, -7.0355e-01, -8.8390e-01, -3.1981e-01, -7.5531e-02,\n",
            "          1.8790e+00, -1.5885e+00, -6.0065e-01, -1.7735e+00],\n",
            "        [-3.7482e-01, -1.4982e+00,  5.6526e-01,  1.2176e+00,  1.6831e+00,\n",
            "          5.7358e-01,  5.6518e-01,  5.1492e-01, -1.0758e+00, -3.0756e-01,\n",
            "          5.6842e-01, -5.3439e-01, -1.0186e+00,  4.5867e-01,  2.0618e+00,\n",
            "          1.6621e+00,  1.2538e-01, -1.1297e-01,  4.3270e-01, -1.1897e+00,\n",
            "          1.0463e+00,  5.2773e-01,  4.4606e-01, -9.7851e-02,  7.3727e-01,\n",
            "         -4.0551e-01,  1.5534e+00, -1.5978e+00, -9.3509e-01, -1.5167e+00,\n",
            "          1.1602e+00, -1.3212e+00, -3.9880e-01, -3.0050e-01,  2.3057e-01,\n",
            "          8.5097e-01, -7.7634e-01,  2.2040e-01,  2.4974e-02, -8.9334e-01,\n",
            "         -1.1605e+00,  6.7717e-02,  8.6879e-04, -2.3544e+00, -5.2858e-01,\n",
            "          3.6558e-01, -1.6231e+00,  5.1306e-01,  3.4410e-01,  4.0646e-01,\n",
            "          1.2078e+00, -2.0837e+00, -2.0578e+00, -1.1028e+00, -1.0727e+00,\n",
            "         -8.8728e-02,  4.7811e-01,  7.8418e-01, -8.3517e-01, -1.5932e-01,\n",
            "         -7.2232e-01, -4.2463e-01, -1.8334e+00,  3.4621e-01],\n",
            "        [ 2.5145e-01,  3.5874e-01,  6.3054e-01, -5.6940e-01,  3.4302e-01,\n",
            "         -1.1641e-01,  3.6488e-01,  7.9961e-01, -6.9947e-02, -4.0269e-01,\n",
            "          7.1883e-01, -2.5941e-01, -5.9297e-01,  2.0418e+00, -6.4374e-01,\n",
            "          6.2997e-01,  1.3110e+00, -5.9882e-02, -9.0418e-01,  3.0698e-01,\n",
            "          1.1504e+00, -6.4538e-01,  3.2145e-01, -1.3783e+00,  3.5766e-01,\n",
            "          4.3677e-02, -4.9336e-01,  9.6948e-01,  1.1441e+00, -7.1974e-01,\n",
            "         -4.0477e-01,  3.1054e-01,  7.4298e-01, -1.5133e-01,  1.0395e+00,\n",
            "          1.0560e+00, -2.1430e+00,  9.3912e-01,  1.5834e+00,  1.0218e+00,\n",
            "          2.6724e-01,  4.5904e-01,  2.7266e-01,  7.7591e-01,  4.8584e-01,\n",
            "          3.3135e-01, -9.2946e-01,  1.7628e-01, -2.7415e-01, -1.1641e+00,\n",
            "          5.6549e-01, -1.6605e+00, -6.3058e-01,  2.0832e-01, -4.9341e-01,\n",
            "          1.8411e+00,  9.8891e-01,  4.1580e-01, -8.5048e-02,  2.0588e-01,\n",
            "          1.2787e+00,  1.9748e-01, -2.0560e+00,  3.7182e-01],\n",
            "        [-3.1675e-01,  1.6324e+00, -1.4191e-01, -2.6248e+00,  6.6466e-01,\n",
            "         -6.5164e-01,  8.8933e-02,  6.7567e-01, -1.8737e+00,  4.4738e-01,\n",
            "          5.1725e-01, -1.2239e+00, -2.6929e-01,  8.1902e-01, -3.4175e-01,\n",
            "          6.6728e-01,  1.9838e+00,  4.8946e-01,  1.0933e+00, -1.5982e+00,\n",
            "         -8.4610e-01, -5.4862e-01,  1.0222e+00, -1.3641e+00,  3.5611e-01,\n",
            "         -7.7454e-01, -8.2259e-01, -4.0561e-01,  1.3575e+00, -1.0371e+00,\n",
            "         -1.3930e+00, -8.2449e-01, -1.6486e+00, -9.2340e-01, -2.2071e+00,\n",
            "          4.3838e-01, -3.2897e-01, -3.2429e-01,  1.6260e-01,  1.0183e+00,\n",
            "          6.5660e-02, -2.9191e-01, -1.1061e+00, -8.3720e-01, -1.2671e+00,\n",
            "          1.6984e+00, -1.8712e+00, -4.4294e-01,  6.0550e-01,  1.0713e+00,\n",
            "          6.2240e-01,  2.4857e-01,  7.9435e-04,  2.0333e-02,  1.3226e+00,\n",
            "          3.2271e-01, -3.2923e-01,  1.0814e+00, -1.2442e+00, -1.6999e+00,\n",
            "          2.1726e-01,  2.0118e+00,  3.3341e-01,  1.0195e+00],\n",
            "        [-4.5742e-01, -1.4090e+00, -2.7274e-01,  5.6697e-01,  8.1706e-01,\n",
            "          1.6067e+00, -2.3110e-01,  8.5294e-01, -1.0336e+00,  5.8150e-01,\n",
            "         -5.7112e-01, -2.5952e-01,  3.0528e-01, -6.5233e-01,  2.8680e-01,\n",
            "         -1.0150e+00,  4.3203e-01,  7.9542e-01,  2.0009e+00,  4.0856e-02,\n",
            "         -7.1705e-01,  1.0462e-01,  7.0509e-01,  5.8301e-01, -1.6104e+00,\n",
            "         -6.0656e-01, -4.7596e-03,  1.3070e+00,  1.9181e+00, -1.9615e+00,\n",
            "         -4.0345e-01, -1.2796e+00, -1.6018e+00, -1.6909e+00, -8.3657e-01,\n",
            "         -3.2969e-01, -4.6610e-01, -2.0782e+00, -1.0002e+00,  8.1653e-01,\n",
            "         -1.6392e+00,  1.3845e-01,  1.1611e+00, -7.1085e-01, -5.9330e-01,\n",
            "         -6.1170e-01,  1.0113e+00, -4.0134e-01, -7.4989e-01,  6.2389e-01,\n",
            "         -1.3126e+00,  1.5551e+00, -1.1301e-01,  5.9305e-01,  6.8310e-01,\n",
            "          8.3773e-01, -2.7916e-01, -1.6708e+00, -5.8529e-01,  1.6090e+00,\n",
            "         -2.6970e-01,  1.0651e-01, -1.1361e-01, -6.7312e-01],\n",
            "        [-6.5056e-01,  1.8107e+00,  2.3767e+00, -3.0720e-01, -1.1412e-01,\n",
            "          1.8990e-01, -4.1700e-01, -8.3593e-01, -4.3352e-01, -5.4850e-01,\n",
            "          1.3119e+00,  1.6935e+00, -1.4890e-01, -1.6550e-01, -3.3278e-01,\n",
            "          5.2776e-01, -1.2205e+00,  3.5870e-01,  9.8007e-01,  3.2444e-02,\n",
            "         -8.7123e-01, -5.0226e-01, -9.1054e-03,  1.1847e+00, -4.9946e-03,\n",
            "         -5.4256e-01, -6.9586e-01,  1.8290e+00,  7.9810e-01, -1.3170e+00,\n",
            "          6.2237e-01, -7.9492e-01, -5.1954e-01, -2.0657e-01, -1.9335e-01,\n",
            "         -8.4115e-01,  1.0698e-01,  4.0610e-01,  7.7414e-01,  5.4816e-01,\n",
            "         -3.3283e-01, -1.9294e-01,  1.1586e+00,  9.2320e-01,  4.7191e-01,\n",
            "         -9.5817e-01, -3.6548e-01, -2.8241e-01, -8.7348e-01, -2.0223e-01,\n",
            "          3.6736e-01, -6.3820e-01, -4.7656e-01,  9.1125e-01, -8.6070e-01,\n",
            "          3.9310e-01,  1.5328e+00,  6.2004e-01,  2.0313e+00, -6.5835e-01,\n",
            "          1.9472e-01,  2.3185e-01, -7.2936e-01,  9.7356e-02],\n",
            "        [-1.3381e-01, -1.4234e+00,  1.8417e+00,  2.4407e+00, -5.7915e-01,\n",
            "         -1.3480e+00,  1.8865e-01,  8.2039e-01, -7.7023e-01, -1.0800e+00,\n",
            "          1.0378e+00, -9.9536e-01, -6.8843e-01, -1.2749e+00,  3.9338e-01,\n",
            "         -1.7164e-01, -8.5773e-01, -9.8330e-01,  2.3972e-01,  6.2195e-02,\n",
            "          1.5256e+00, -7.0711e-01,  5.4725e-01, -6.6470e-01,  3.7964e-01,\n",
            "          6.9319e-01,  1.7270e+00,  3.9232e-01, -1.0650e+00, -1.8249e-01,\n",
            "          7.2623e-01, -9.5977e-01,  2.6486e-01,  8.9164e-01,  1.2059e+00,\n",
            "          9.5426e-01, -6.6111e-01,  1.0849e+00,  1.6200e+00,  1.0452e+00,\n",
            "         -1.7584e+00, -1.5029e+00, -6.5546e-01,  7.0259e-01, -1.1133e-02,\n",
            "         -1.6221e+00, -9.7451e-02, -8.6219e-01,  1.0898e+00, -8.1752e-01,\n",
            "         -8.2980e-02,  5.3037e-02, -1.1322e+00, -1.3710e+00,  9.6588e-01,\n",
            "          3.6741e-01,  6.4383e-01,  5.0286e-01,  3.9132e-01,  5.4563e-01,\n",
            "         -1.4652e+00, -6.6810e-01, -8.5130e-01, -1.9147e+00],\n",
            "        [ 1.3188e+00,  2.0149e+00,  1.9583e+00,  1.0488e+00, -7.8395e-01,\n",
            "          1.0246e+00,  1.1516e+00,  2.0809e+00,  1.0501e+00,  1.0719e-01,\n",
            "         -5.7425e-01, -2.4024e+00,  4.6734e-02,  1.2849e+00,  6.7883e-01,\n",
            "          6.8434e-01,  1.2968e+00,  1.9488e-01,  4.5652e-01, -1.5786e+00,\n",
            "         -2.6289e+00, -1.2490e-01, -1.3015e+00, -1.6931e-02,  2.5584e-01,\n",
            "          2.5589e-01, -1.2275e+00,  3.5449e-01,  1.1128e-02, -3.5994e-01,\n",
            "         -1.8815e-02,  4.1507e-01,  7.3757e-01,  8.3845e-02,  8.2766e-01,\n",
            "         -1.7059e-01,  3.2195e-01,  1.4441e+00,  2.0769e+00, -3.7155e-01,\n",
            "          1.0933e+00, -1.6600e-01, -8.4692e-02, -1.6792e+00, -1.7504e+00,\n",
            "          5.5032e-01, -1.3834e-01,  4.3578e-01,  8.4043e-01, -9.9320e-01,\n",
            "         -5.4003e-01,  1.7955e+00,  2.5340e-01,  1.4647e+00,  3.0546e-01,\n",
            "          5.2001e-01, -6.0406e-02, -1.5673e+00, -7.0070e-01,  5.9034e-01,\n",
            "         -2.2104e-01,  6.8574e-02,  9.4808e-01,  5.7196e-01]])\n"
          ]
        }
      ],
      "source": [
        "seq_length= 10\n",
        "d_model= 64\n",
        "\n",
        "toy_input_seq= torch.randn(seq_length, d_model)\n",
        "print(toy_input_seq.shape)\n",
        "print(toy_input_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SqSp-IGQIMbj"
      },
      "outputs": [],
      "source": [
        "class SelfAttention_Feedforward_normalisation(nn.Module):\n",
        "  def __init__(self, d_model ):\n",
        "    super(SelfAttention_Feedforward_normalisation, self).__init__ ()\n",
        "\n",
        "    self.d_model=d_model\n",
        "    self.w_query=nn.Linear(d_model, d_model)\n",
        "    self.w_key=nn.Linear(d_model, d_model)\n",
        "    self.w_value=nn.Linear(d_model,d_model)\n",
        "\n",
        "    self.l1=nn.Linear(d_model, d_model)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.l2=nn.Linear(d_model,d_model)\n",
        "\n",
        "    self.layer_norm=nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    query_vector =self.w_query(x)  #XW_q\n",
        "    key_vector= self.w_key(x) #XW_k\n",
        "    value_vector= self.w_value(x) #XW_v\n",
        "\n",
        "    E= torch.matmul(query_vector, key_vector.transpose(-2,-1))/math.sqrt(self.d_model)\n",
        "    attention_weight= torch.softmax(E, dim=-1)\n",
        "    output_vec=torch.matmul(attention_weight, value_vector)\n",
        "\n",
        "    norm1=self.layer_norm(x+output_vec)\n",
        "\n",
        "    out=self.l1(norm1)\n",
        "    out=self.relu(out)\n",
        "    out=self.l2(out)\n",
        "\n",
        "    norm2=self.layer_norm(out+norm1)\n",
        "\n",
        "    return norm2,attention_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XcadRvwqY-Z9"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, d_model, num_layers):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.layer= nn.ModuleList([\n",
        "            SelfAttention_Feedforward_normalisation(d_model)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    attn_weights_all = []\n",
        "    for layer in self.layer:\n",
        "        x, attention_weight = layer(x)\n",
        "        attn_weights_all.append(attention_weight)\n",
        "        return x, attention_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aYIoGPh7cnRX"
      },
      "outputs": [],
      "source": [
        "num_layer=[1,2,4]\n",
        "attentions={}\n",
        "results={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15cEN6B5ddXm",
        "outputId": "fa35f44a-663d-4da5-d3fa-1030e623dae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'1 layer': tensor([0.1431, 0.1340, 0.0672, 0.0862, 0.1688, 0.0989, 0.0730, 0.0633, 0.0641,\n",
            "        0.1012], grad_fn=<SelectBackward0>)}\n",
            "{'1 layer': tensor([0.1431, 0.1340, 0.0672, 0.0862, 0.1688, 0.0989, 0.0730, 0.0633, 0.0641,\n",
            "        0.1012], grad_fn=<SelectBackward0>), '2 layer': tensor([0.0650, 0.0908, 0.0358, 0.1028, 0.0945, 0.1209, 0.2061, 0.0972, 0.1457,\n",
            "        0.0411], grad_fn=<SelectBackward0>)}\n",
            "{'1 layer': tensor([0.1431, 0.1340, 0.0672, 0.0862, 0.1688, 0.0989, 0.0730, 0.0633, 0.0641,\n",
            "        0.1012], grad_fn=<SelectBackward0>), '2 layer': tensor([0.0650, 0.0908, 0.0358, 0.1028, 0.0945, 0.1209, 0.2061, 0.0972, 0.1457,\n",
            "        0.0411], grad_fn=<SelectBackward0>), '4 layer': tensor([0.0717, 0.1297, 0.0829, 0.0996, 0.0880, 0.0686, 0.0978, 0.0716, 0.1494,\n",
            "        0.1406], grad_fn=<SelectBackward0>)}\n"
          ]
        }
      ],
      "source": [
        "for i in num_layer:\n",
        "  model=TransformerEncoder(d_model=d_model, num_layers=i)\n",
        "  out, attn = model(toy_input_seq)\n",
        "  results[f\"{i} layers\"] = out\n",
        "  attentions[f\"{i} layer\"] = attn[-1]\n",
        "  print(attentions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
