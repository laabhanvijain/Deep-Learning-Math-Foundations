{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q3.** In the neural network shown in the figure, all the neurons use **negative sigmoid units** meaning that their output is computed as $o = -\\frac{1}{1 + e^{-y}}$. Additionally, the network uses a **non-standard error function** $E = \\frac{1}{2}(2t - 2o)^2$. If the target outputs are $y_2 = 2$ and $y_3 = 0.5$, and the learning rate is $0.2$, determine all the weights after one epoch.\n",
        "\n",
        "![Untitled Diagram.drawio](https://hackmd.io/_uploads/HJrqwTOMex.png)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhuFEGENscUB"
      },
      "source": [
        "### **3.**\n",
        "Given: $x=2, w_1=4, w_2=3, w_3=2, y_2=2, y_3=0.5, Î±=0.2 $\n",
        "<br/>\n",
        "negative sigmoid: $o(x) = -\\frac{1}{1 + e^{-x}} $\n",
        "</br>\n",
        "\n",
        "$a_1=o(w_1 x)$ </br>\n",
        "$= -0.9996646$ </br>\n",
        "$a_2=o(w_2 a_1)$ </br>\n",
        "$= -0.0474713$ </br>\n",
        "$a_3=o(w_3 a_1)$ </br>\n",
        "$= -0.1192734$ </br>\n",
        " </br>\n",
        "Loss: </br>\n",
        "$E_2= \\frac{1}{2}(2y_2-2a_2)^2$ </br>\n",
        "$E_3= \\frac{1}{2}(2y_3-2a_3)^2$ </br>\n",
        " </br>\n",
        " To do backprop: </br>\n",
        " Derivative of $o(x)$ </br>\n",
        " Similar to sigmoid, only sign difference: </br>\n",
        " $ \\frac{do(x)}{dx} = o(x).(1+o(x)) $ </br>\n",
        " </br>\n",
        "Gradient of loss wrt weights: </br>\n",
        "$ \\frac{\\partial E_2}{\\partial w_2} = \\frac{\\partial E_2}{\\partial a_2}. \\frac{\\partial a_2}{\\partial o} . \\frac{\\partial o}{\\partial w_2}$ </br>\n",
        "$= -4(y_2-a_2). a_2(1+a_2).a_1$ </br>\n",
        "$=-0.37020$\n",
        "\n",
        "$ \\frac{\\partial E_3}{\\partial w_3} = \\frac{\\partial E_3}{\\partial a_3}. \\frac{\\partial a_3}{\\partial o} . \\frac{\\partial o}{\\partial w_3}$ </br>\n",
        "$= -4(y_3-a_3). a_3(1+a_3).a_1$ </br>\n",
        "$=-0.26012$\n",
        "\n",
        "$ \\frac{\\partial E}{\\partial w_1} = \\frac{\\partial E}{\\partial a_1}. \\frac{\\partial a_1}{\\partial o} . \\frac{\\partial o}{\\partial w_1}$ </br>\n",
        "$= (\\frac{\\partial E_2}{\\partial a_1}+ \\frac{\\partial E_3}{\\partial a_1}).\\frac{\\partial a_1}{\\partial o} . \\frac{\\partial o}{\\partial w_1}$ </br>\n",
        "$= (\\frac{\\partial E_2}{\\partial a_2}. \\frac{\\partial a_2}{\\partial a_1} + \\frac{\\partial E_3}{\\partial a_3}. \\frac{\\partial a_3}{\\partial a_1}).\\frac{\\partial a_1}{\\partial w_1} $ </br>\n",
        "$= [(-4(y_2 - a_2) .  a_2 . (1 + a_2). w_2) + (-4(y_3 - a_3) .  a_3 . (1 + a_3). w_3)]. a_1.(1+a_1).x$ </br>\n",
        "$= (1.110985 + 0.520423)(-0.0006706)$ </br>\n",
        "$= -0.001094$\n",
        "\n",
        "\n",
        "\n",
        "**Final weights:** \\\\\n",
        "$w_1^{'} = w_1 - \\alpha\\frac{\\partial E}{\\partial w_1} $ </br>\n",
        "$= 4- 0.2(-0.001094)= 4.000218$ </br>\n",
        "<br/>\n",
        "$w_2^{'} = w_2 - \\alpha\\frac{\\partial E_2}{\\partial w_2}$ </br>\n",
        "$= 3- 0.2(-0.37020) = 3.07404$ </br>\n",
        "<br/>\n",
        "$w_3^{'} = w_3 - \\alpha\\frac{\\partial E_3}{\\partial w_3}$ </br>\n",
        "$= 2- 0.2(-0.26012)= 2.05202$ </br>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
